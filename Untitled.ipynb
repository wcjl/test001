{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'index_e_26_no' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-d7c9f7d802d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mbar_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mindexe_26_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mindex_e_equator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_e_26_no\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbar_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_e_26_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me_26_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'01_26_no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_e_26_no' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "matrics = ('AUC_Judd','CC','KLD','NSS','SIM')\n",
    "e_26 = [0.753575,0.732756,0.300413,1.112815,0.711277]\n",
    "e_26_test = [0.754249,0.735635,0.301553,1.117340,0.712292]\n",
    "\n",
    "e_26_no = [0.75334,0.733094,0.306163,1.112624,0.711132]\n",
    "e_11_16_40 = [0.755525,0.738307,0.293626,1.122232,0.715231]\n",
    "e_5_8_40 = [0.755715,0.737063,0.300144,1.121519,0.715048]\n",
    "e_7_10_26 = [0.752538,0.722281,0.309678,1.096839,0.708639]\n",
    "e_5_6_26 = [0.742148,0.689310,0.352269,1.048817,0.685959]\n",
    "\n",
    "e_equator = [0.743872,0.702863,0.344610,1.066033,0.689668]\n",
    "\n",
    "\n",
    "bar_width = 0.3\n",
    "indexe_26_no = np.arange(len(matrics))\n",
    "index_e_equator = index_e_26_no + bar_width\n",
    "\n",
    "plt.bar(index_e_26_no, height=e_26_no, width=bar_width, color='y', label='01_26_no')\n",
    "plt.bar(index_e_equator, height=e_equator, width=bar_width, color='g', label='01_26_equator')\n",
    " \n",
    "plt.legend()\n",
    "plt.xticks(index_e_26_no + bar_width/2, matrics) \n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for \"Context-Gated Convolution\"\n",
    "# ECCV 2020\n",
    "# Xudong Lin*, Lin Ma, Wei Liu, Shih-Fu Chang\n",
    "# {xudong.lin, shih.fu.chang}@columbia.edu, forest.linma@gmail.com, wl2223@columbia.edu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "import numpy as np  \n",
    "\n",
    "class Conv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, groups, bias)\n",
    "        # for convolutional layers with a kernel size of 1, just use traditional convolution\n",
    "        if kernel_size == 1:\n",
    "            self.ind = True\n",
    "        else:\n",
    "            self.ind = False            \n",
    "            self.oc = out_channels\n",
    "            self.ks = kernel_size\n",
    "            \n",
    "            # the target spatial size of the pooling layer\n",
    "            ws = kernel_size\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d((ws,ws))\n",
    "            \n",
    "            # the dimension of the latent repsentation\n",
    "            self.num_lat = int((kernel_size * kernel_size) / 2 + 1)\n",
    "            \n",
    "            # the context encoding module\n",
    "            self.ce = nn.Linear(ws*ws, self.num_lat, False)    \n",
    "            print(\"self.num_lat:\", self.num_lat)\n",
    "            print(\"ws:\",ws)        \n",
    "            self.ce_bn = nn.BatchNorm1d(in_channels)\n",
    "            self.ci_bn2 = nn.BatchNorm1d(in_channels)\n",
    "            \n",
    "            # activation function is relu\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "            \n",
    "            \n",
    "            # the number of groups in the channel interacting module\n",
    "            if in_channels // 16:\n",
    "                self.g = 16\n",
    "            else:\n",
    "                self.g = in_channels\n",
    "            # the channel interacting module    \n",
    "            self.ci = nn.Linear(self.g, out_channels // (in_channels // self.g), bias=False)\n",
    "            print(\"out_channels // (in_channels // self.g):\",out_channels / (in_channels / self.g))\n",
    "            print(\"g:\",self.g)\n",
    "            print(\"outchann:\",self.out_channels)\n",
    "            print(\"inchann:\",self.in_channels)\n",
    "            print(\"ci\",self.ci)\n",
    "            self.ci_bn = nn.BatchNorm1d(out_channels)\n",
    "            \n",
    "            # the gate decoding module\n",
    "            self.gd = nn.Linear(self.num_lat, kernel_size * kernel_size, False)\n",
    "            self.gd2 = nn.Linear(self.num_lat, kernel_size * kernel_size, False)\n",
    "            \n",
    "            # used to prrepare the input feature map to patches\n",
    "            self.unfold = nn.Unfold(kernel_size, dilation, padding, stride)\n",
    "            \n",
    "            # sigmoid function\n",
    "            self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        # for convolutional layers with a kernel size of 1, just use traditional convolution\n",
    "        if self.ind:\n",
    "            return F.conv2d(x, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "        else:\n",
    "            b, c, h, w = x.size()\n",
    "            print(\"x:\",x.size())\n",
    "            weight = self.weight\n",
    "            # allocate glbal information\n",
    "            gl = self.avg_pool(x).view(b,c,-1)\n",
    "            # context-encoding module\n",
    "            out = self.ce(gl)\n",
    "            # use different bn for the following two branches\n",
    "            ce2 = out\n",
    "            print(\"ce:\",ce2.size())\n",
    "            out = self.ce_bn(out)\n",
    "            print(\"ceout:\",out.size())\n",
    "            out = self.act(out)\n",
    "            \n",
    "\n",
    "            # gate decoding branch 1\n",
    "            out = self.gd(out)\n",
    "            print(\"ceout:\",out.size())\n",
    "\n",
    "            # channel interacting module\n",
    "            print(self.g)\n",
    "            if self.g >3:\n",
    "                # grouped linear\n",
    "                print(self.ci_bn2(ce2).\\\n",
    "                                      view(b, c//self.g, self.g, -1).size())\n",
    "                oc = self.ci(self.act(self.ci_bn2(ce2).\\\n",
    "                                      view(b, c//self.g, self.g, -1).transpose(2,3))).transpose(2,3).contiguous()\n",
    "                print(\"oc:\",oc.size())\n",
    "            else:\n",
    "                # linear layer for resnet.conv1\n",
    "                \n",
    "                oc = self.ci(self.act(self.ci_bn2(ce2).transpose(2,1))).transpose(2,1).contiguous() \n",
    "                \n",
    "            oc = oc.view(b,self.oc,-1) \n",
    "            oc = self.ci_bn(oc)\n",
    "            oc = self.act(oc)\n",
    "            # gate decoding branch 2\n",
    "            oc = self.gd2(oc)   \n",
    "            # produce gate\n",
    "            out = self.sig(out.view(b, 1, c, self.ks, self.ks) + oc.view(b, self.oc, 1, self.ks, self.ks))\n",
    "            print(\"out:\",out.size())\n",
    "            # unfolding input feature map to patches\n",
    "            x_un = self.unfold(x)\n",
    "            print(\"x_un:\",x_un.size())\n",
    "            b, _, l = x_un.size()\n",
    "            # gating\n",
    "            out = (out * weight.unsqueeze(0)).view(b, self.oc, -1)\n",
    "            print(\"out:\",out.size())\n",
    "            # currently only handle square input and output\n",
    "            print(torch.matmul(out,x_un).view(b, self.oc, int(np.sqrt(l)),-1).size())\n",
    "            return torch.matmul(out,x_un).view(b, self.oc, int(np.sqrt(l)), -1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randint(0,100,(1,16,16,32))\n",
    "b = torch.from_numpy(b).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "self.num_lat: 5\nws: 3\nout_channels // (in_channels // self.g): 16.0\ng: 16\noutchann: 16\ninchann: 16\nci Linear(in_features=16, out_features=16, bias=False)\nx: torch.Size([1, 16, 16, 32])\nce: torch.Size([1, 16, 5])\nceout: torch.Size([1, 16, 5])\nceout: torch.Size([1, 16, 9])\n16\ntorch.Size([1, 1, 16, 5])\noc: torch.Size([1, 1, 16, 5])\nout: torch.Size([1, 16, 16, 3, 3])\nx_un: torch.Size([1, 144, 420])\nout: torch.Size([1, 16, 144])\ntorch.Size([1, 16, 20, 21])\n"
     ]
    }
   ],
   "source": [
    "conv2 = Conv2d(16,16,kernel_size=3)\n",
    "out = conv2(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('anaconda3-2020.02': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "e01f2aa1e27acc7e11b146e9c3f3b72c05d0c1a783a47e5e05ce24a74cbd73e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}